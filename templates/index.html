<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>MedWaste Guardian</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f4f4f4;
      text-align: center;
      margin: 0;
      padding: 0;
    }
    h1 {
      background-color: #4CAF50;
      color: white;
      padding: 20px;
    }
    .container {
      margin: 20px auto;
      padding: 20px;
      max-width: 800px;
      background: white;
      box-shadow: 0 0 10px rgba(0,0,0,0.1);
    }
    #response, #sttText, #imageResults {
      margin: 10px 0;
      padding: 10px;
      background: #f0f0f0;
      border: 1px solid #ccc;
      min-height: 40px;
      white-space: pre-wrap;
    }
    video, canvas, img {
      margin: 10px 0;
      max-width: 100%;
    }
    button {
      margin: 10px;
      padding: 10px 20px;
      background: #4CAF50;
      color: white;
      border: none;
      cursor: pointer;
    }
    button:hover {
      background: #45a049;
    }
  </style>
</head>
<body>
  <h1>MedWaste Guardian</h1>
  <div class="container">
    <button onclick="startRecording()">üé§ Start Speech Input</button>
    <div id="sttText">Speech transcription will appear here...</div>
    <div id="response">AI Response will appear here...</div>

    <hr />

    <input type="file" id="imageInput" accept="image/*">
    <button onclick="sendImage()">üñºÔ∏è Classify Waste</button>
    <div id="imageResults">Image classification results will appear here...</div>
    <img id="preview" style="display:none; max-width:300px;" />

  </div>

  <script>
    // Speech Input to RAG
    function startRecording() {
      const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      recognition.lang = 'en-US';
      recognition.interimResults = false;

      recognition.onresult = function(event) {
        const transcript = event.results[0][0].transcript;
        document.getElementById("sttText").innerText = transcript;

        fetch('http://127.0.0.1:8000/query/', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ query: transcript })
        })
        .then(res => res.json())
        .then(data => {
          document.getElementById("response").innerText = data.response;
        })
        .catch(err => {
          document.getElementById("response").innerText = `Error: ${err.message}`;
        });
      };

      recognition.onerror = function(err) {
        document.getElementById("sttText").innerText = `Error accessing microphone: ${err.message}`;
      };

      recognition.start();
    }

    // Image Upload and Classification
    function sendImage() {
      const input = document.getElementById("imageInput");
      const file = input.files[0];
      if (!file) {
        alert("Please select an image first.");
        return;
      }

      const formData = new FormData();
      formData.append("image", file);

      fetch("/predict", {
        method: "POST",
        body: formData
      })
      .then(response => response.json())
      .then(data => {
        const preview = document.getElementById("preview");
        preview.src = URL.createObjectURL(file);
        preview.style.display = "block";

        const resultsDiv = document.getElementById("imageResults");
        if (data.results && data.results.length > 0) {
          resultsDiv.innerText = data.results.map(
            r => `Label: ${r.class}, Confidence: ${r.confidence.toFixed(2)}, Box: (${r.box.join(", ")})`
          ).join("\n");
        } else {
          resultsDiv.innerText = "No objects detected.";
        }
      })
      .catch(err => {
        document.getElementById("imageResults").innerText = `Error: ${err.message}`;
      });
    }
  </script>
</body>
</html>
